<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Reinforcement Learning | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="UCB 算法Upper Confidence Bound Algorithm, 是一个多臂老虎机算法，用于解决探索与利用的问题。 设计初衷Exploration the commit 算法，对每个动作探索相同的次数，即使对于已经比较差的动作，也会尝试m次，从而造成浪费 算法描述UCB会在每轮中对每个动作进行评分，之后选择评分最高的动作进行尝试，评分的设计目标如下： (1) 如果一个动作探索的次数比">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning">
<meta property="og:url" content="http://example.com/2023/09/13/Reinforcement-Learning/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="UCB 算法Upper Confidence Bound Algorithm, 是一个多臂老虎机算法，用于解决探索与利用的问题。 设计初衷Exploration the commit 算法，对每个动作探索相同的次数，即使对于已经比较差的动作，也会尝试m次，从而造成浪费 算法描述UCB会在每轮中对每个动作进行评分，之后选择评分最高的动作进行尝试，评分的设计目标如下： (1) 如果一个动作探索的次数比">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-09-12T18:35:39.000Z">
<meta property="article:modified_time" content="2023-09-13T06:38:44.620Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Reinforcement-Learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/13/Reinforcement-Learning/" class="article-date">
  <time class="dt-published" datetime="2023-09-12T18:35:39.000Z" itemprop="datePublished">2023-09-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Reinforcement Learning
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="UCB-算法"><a href="#UCB-算法" class="headerlink" title="UCB 算法"></a>UCB 算法</h2><p>Upper Confidence Bound Algorithm, 是一个多臂老虎机算法，用于解决探索与利用的问题。</p>
<h3 id="设计初衷"><a href="#设计初衷" class="headerlink" title="设计初衷"></a>设计初衷</h3><p>Exploration the commit 算法，对每个动作探索相同的次数，即使对于已经比较差的动作，也会尝试m次，从而造成浪费</p>
<h3 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h3><p>UCB会在每轮中对每个动作进行评分，之后选择评分最高的动作进行尝试，评分的设计目标如下：<br> (1) 如果一个动作探索的次数比别的动作少，则它更该被选中（评分更高）<br> (2) 如果一个动作的平均收益比别的动作高，则它更该被选中（评分更高）</p>
<h3 id="什么是置信区间"><a href="#什么是置信区间" class="headerlink" title="什么是置信区间"></a>什么是置信区间</h3><script type="math/tex; mode=display">\mathbb{P}(\hat{\mu} - \epsilon < \mu < \hat{\mu} + \epsilon) = \lambda</script><p>其中，$\hat{\mu}$是样本均值，$\epsilon$是探索奖励（有鼓励探索的作用），$\lambda$是置信度</p>
<h3 id="直观结果"><a href="#直观结果" class="headerlink" title="直观结果"></a>直观结果</h3><p> (1) 探索次数少，样本容量小，$\epsilon$变大，置信区间上界变大，更应该被选中<br> (2) $\hat{\mu}$ 收到以往探索受到奖励的大小变化，如果以往大，那么置信区间的上界就大<br> (3) 不会过多探索较差的动作，优先探索目前好的（UCB最大的）<br>置信水平会随着探索次数增加而减小，即$\lambda$会随着探索次数增加而减小，这样可以保证算法会收敛到最优解</p>
<h2 id="UCB的推导"><a href="#UCB的推导" class="headerlink" title="UCB的推导"></a>UCB的推导</h2><p>UBC算法只关心上界：</p>
<script type="math/tex; mode=display">\mathbb{P}(\mu < \hat{\mu} + \epsilon) = \lambda</script><script type="math/tex; mode=display">\Rightarrow\mathbb{P}(\mu - \hat{\mu} \geq \epsilon) = 1 - \lambda = \delta</script><p>保守置信区间：</p>
<script type="math/tex; mode=display">\mathbb{P}(\mu - \hat{\mu} \geq \epsilon) = 1 - \lambda \leq \delta</script><p>依据霍夫丁界：</p>
<script type="math/tex; mode=display">\mathbb{P}(\mu - \hat{\mu} \geq \epsilon) \leq exp\left(-\frac{h\epsilon^2}{2}\right)</script><script type="math/tex; mode=display">\Rightarrow \delta = exp\left(-\frac{h\epsilon^2}{2}\right)</script><script type="math/tex; mode=display">\epsilon = \sqrt{\frac{2log(1/ \delta)}{n}}</script><p>得到每个动作的UCB：</p>
<script type="math/tex; mode=display">UCB_i(t-1, \delta) \triangleq 
\left\{
  \begin{aligned}
    &\infty &&\text{if}\ T_i(t-1) = 0 \\
    &\hat{\mu}_i(t-1) + \sqrt{\frac{2log(1/ \delta)}{T_i(t-1)}} &&otherwise \\
  \end{aligned}
\right.</script><p>算法会优先选择没有探索过的动作</p>
<h2 id="UCB1的实现"><a href="#UCB1的实现" class="headerlink" title="UCB1的实现"></a>UCB1的实现</h2><p>input k(动作个数)</p>
<p>for i in n do:<br>$\quad$   choose $A_t = \arg\max_{i\in [k]} UCB_i(t-1, \frac{1}{t-1})$<br>$\quad$    record $X_t$，update UCB<br>end for</p>
<p>这里的$A_t$是动作的索引，$X_t$是动作的收益，UCB是每个动作的UCB值</p>
<h3 id="UCB1算法的理论分析"><a href="#UCB1算法的理论分析" class="headerlink" title="UCB1算法的理论分析"></a>UCB1算法的理论分析</h3><p>Finite-time Analysis of the Multiarmed Bandit Problem</p>
<h4 id="理论分析算法的分析"><a href="#理论分析算法的分析" class="headerlink" title="理论分析算法的分析"></a>理论分析算法的分析</h4><ol>
<li>最简单的： 集中不等式</li>
<li>一步一步拆，看看能不能推出一个次线性增长的界</li>
</ol>
<h4 id="大体框架"><a href="#大体框架" class="headerlink" title="大体框架"></a>大体框架</h4><p>算法产生了遗憾 $\leftrightarrow $ 算法选择了次优的动作 $\leftrightarrow$ 次优动作的指标高于其他的所有动作 $\rightarrow$ 次优动作的指标高于最优动作的指标<br>$\Rightarrow$ 最优动作的指标过低，次优动作的指标过高。</p>
<p>我们知道，每个状态的UCB指标是分为两部分来计算的，一部分是累积奖励的平均值，一部分是探索奖励，我们可以通过分析这两部分来分析算法的遗憾。<br>探索奖励会随着一个动作探索次数的增加而衰减，因此它不会影响太多，只会影响前面有限轮。关注一个算法的遗憾是不是次线性增长，关注的是长期的增长速度，不会受到前面有限轮遗憾的影响。</p>
<p>```mermaid<br>graph TD;<br>A—&gt;B;<br>A—&gt;C;<br>B—&gt;D;<br>C—&gt;D;</p>
<p>```gantt<br>dateFormat  YYYY-MM-DD<br>title Adding Gantt diagram functionality to Mermaid</p>
<p>section A section<br>Completed task            :done,    des1, 2014-01-06,2014-01-08<br>Active task               :active,  des2, 2014-01-09, 3d<br>Future task               :         des3, after des2, 5d<br>Future task2              :         des4, after des3, 5d</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/09/13/Reinforcement-Learning/" data-id="clmhdfo970001y9qw9sej3wsj" data-title="Reinforcement Learning" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2023/09/13/Measure%20Theory/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Measure Theory</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/09/13/Reinforcement-Learning/">Reinforcement Learning</a>
          </li>
        
          <li>
            <a href="/2023/09/13/Measure%20Theory/">Measure Theory</a>
          </li>
        
          <li>
            <a href="/2023/09/13/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="//cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>